{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ODENet:\n",
    "    https://arxiv.org/abs/1806.07366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge_kutta(f, t0, t1, x0, h):\n",
    "    t = t0\n",
    "    x = x0\n",
    "    n_steps = m.ceil((t0-t1).abs() / h)\n",
    "    for i in range(n_steps):\n",
    "        k1 = f(x, t)\n",
    "        k2 = f(x + 0.5*h*k1, t + 0.5*h)\n",
    "        k3 = f(x + 0.5*h*k2, t + 0.5*h)\n",
    "        k4 = f(x + h*k3, t + h)\n",
    "\n",
    "        x = x + h*(k1+2*k2 + 2*k3 + k4) / 6\n",
    "        t = (t+h) if (t0 < t1) else (t-h)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#super class for function\n",
    "class ODE_Function(torch.nn.Module):\n",
    "    \n",
    "    #return f and all function for backward (a*df/dz, a*df/dp, a*df/dt) \n",
    "    def forward_and_grads(self, z_in, t_in, grad_outputs):\n",
    "        f = self.forward(z_in, t_in)\n",
    "        bs = z_in.shape[0]\n",
    "        a = grad_outputs\n",
    "\n",
    "        z_in.requires_grad = True\n",
    "        t_in.requires_grad = True\n",
    "\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (f,), (z_in, t_in) + tuple(self.parameters()), grad_outputs=(a), allow_unused=True, retain_graph=True )\n",
    "        \n",
    "        if adfdp is not None:\n",
    "            for i in range(len(adfdp)):\n",
    "                if adfdp[i] is None:\n",
    "                    adfdp[i] = torch.Tensor(*list(self.parameters())[i].shape).to(z_in)\n",
    "            adfdp = torch.cat([p_grads.flatten() for p_grads in adfdp])\n",
    "            \n",
    "        return f, adfdz, adfdp, adfdt\n",
    "    \n",
    "    def flat_params(self):\n",
    "        params = []\n",
    "        for p in self.parameters():\n",
    "            params.append(p.flatten())\n",
    "        return torch.cat(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, params, f, h):\n",
    "        assert isinstance(f, ODE_Function)\n",
    "        z_shape = z0.shape\n",
    "        time_len = t.shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.Tensor(time_len, *z_shape).to(z0)\n",
    "            z[0] = z0\n",
    "            for i in range(time_len - 1):\n",
    "                z[i+1] = runge_kutta(f, t[i], t[i+1], z[i], h)\n",
    "      \n",
    "        ctx.f = f\n",
    "        ctx.save_for_backward(z.clone(), t, params)\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz, h):\n",
    "        f = ctx.f\n",
    "        z, t, params = ctx.saved_tensors\n",
    "        time_len, *z_shape = z.shape\n",
    "        z_dim = np.prod(z_shape)\n",
    "        params_dim = params.shape[0]\n",
    "        \n",
    "        #Right part of the augmented system\n",
    "        def aug_dynamics(aug_v, t):\n",
    "            '''\n",
    "                dim aug_v = 2*z_dim + params_dim + 1\n",
    "                aug_v = (z, a, params, t)\n",
    "            '''\n",
    "            z, a = aug_v[0:z_dim], aug_v[z_dim:2*z_dim]\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "\n",
    "                f_eval, adfdz, adfdp, adfdt = f.forward_and_grads(z, t, a)\n",
    "                adfdz = adfdz if adfdz is not None else torch.zeros(*z_shape)\n",
    "                adfdp = adfdp if adfdp is not None else torch.zeros(params_dim)\n",
    "                adfdt = adfdt if adfdt is not None else torch.zeros(1)\n",
    "\n",
    "                adfdz = adfdz.to(z)\n",
    "                adfdp = adfdp.to(z)\n",
    "                adfdt = adfdt.to(z)\n",
    "\n",
    "            return torch.cat((f_eval, -adfdz, -adfdp, -adfdt))\n",
    "        \n",
    "        dLdz = dLdz.view(time_len, z_dim)\n",
    "        with torch.no_grad():\n",
    "            adj_z = torch.zeros(z_dim).to(dLdz)\n",
    "            adj_params = torch.zeros(params_dim).to(dLdz)\n",
    "            adj_t = torch.zeros(time_len).to(dLdz)\n",
    "            \n",
    "            for i in range(time_len-1, 0, -1):\n",
    "                f_i = f(z[i], t[i]).to(dLdz)\n",
    "                dLdz_i = dLdz[i].to(dLdz)\n",
    "                dLdt_i =torch.matmul( dLdz_i, f_i ).to(dLdz)\n",
    "                \n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i] -= dLdt_i\n",
    "                z_i = z[i].to(dLdz)\n",
    "                aug_v = torch.cat((z_i, adj_z, torch.zeros(params_dim).to(z), adj_t[i].unsqueeze(0)))\n",
    "                \n",
    "                aug_solution = runge_kutta(aug_dynamics, t[i], t[i-1], aug_v, h)\n",
    "                \n",
    "                adj_z[:] = aug_solution[z_dim:2*z_dim]\n",
    "                adj_params[:] = aug_solution[2*z_dim:2*z_dim + params_dim]\n",
    "                adj_t[i - 1] = aug_solution[2*z_dim + params_dim:]\n",
    "                \n",
    "                del aug_v, aug_solution\n",
    "                \n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.matmul( dLdz_0, f_i)\n",
    "            \n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] -= dLdt_0\n",
    "        return adj_z, adj_t, adj_params, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper class of ODEAdjoint \n",
    "class ODELayer(torch.nn.Module):\n",
    "    def __init__(self, f, h):\n",
    "        super(ODELayer, self).__init__()\n",
    "        assert isinstance(f, ODE_Function)\n",
    "        self.f = f\n",
    "        self.h = h\n",
    "        \n",
    "    def forward(self, z0, t=torch.Tensor([0., 1.])):\n",
    "        t = t.to(z0)\n",
    "        z = ODEAdjoint.apply(z0, t, self.f.flat_params(), self.f, self.h)       \n",
    "        return z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
